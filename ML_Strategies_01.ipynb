{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MikChristensen/Algo-Trading-Examples/blob/main/ML_Strategies_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXkp-YDap9W0"
      },
      "source": [
        "# LSTM\n",
        "\n",
        "https://github.com/soms98/Stock-Price-Prediction-Time-Series-LSTM-Model-Keras-Tensorflow/blob/master/HDFC.ipynb\n",
        "https://www.youtube.com/watch?v=CbTU92pbDKw&t=1312s\n",
        "\n",
        "https://medium.com/mlearning-ai/multivariate-time-series-forecasting-using-rnn-lstm-8d840f3f9aa7\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acwplh7bp9W3",
        "outputId": "ad996bba-e8f4-47c2-9cb4-f1cedf24adee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'talib'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f946454ca2bb>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Preprocessing data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtalib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabstract\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRobustScaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m \u001b[0;31m# Mathematical functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'talib'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "\n",
        "# Import tensorflow packages\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.callbacks import EarlyStopping # EarlyStopping during model training\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error # Packages for measuring model performance / errors\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "# Displaying data\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "\n",
        "# Preprocessing data\n",
        "from talib import abstract\n",
        "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
        "import math # Mathematical functions\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ta-lib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5gZTHlX3w8i",
        "outputId": "82f45816-33bc-4793-bd73-c312be6f0a7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ta-lib\n",
            "  Downloading TA-Lib-0.4.32.tar.gz (368 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/368.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m358.4/368.5 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m368.5/368.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ta-lib) (1.25.2)\n",
            "Building wheels for collected packages: ta-lib\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for ta-lib \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for ta-lib (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for ta-lib\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build ta-lib\n",
            "\u001b[31mERROR: Could not build wheels for ta-lib, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rjtwSkDp9W5"
      },
      "source": [
        "## Project variabels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMaA84-9p9W5"
      },
      "outputs": [],
      "source": [
        "symbol = \"EURUSD=X\"\n",
        "interval ='5m'\n",
        "period = '1mo'\n",
        "\n",
        "train_size_fraction = 0.70"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJubcbggp9W6"
      },
      "source": [
        "## Download data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFYePqPLp9W6"
      },
      "outputs": [],
      "source": [
        "def getHistoryYfinance(symbol, interval, period):\n",
        "\n",
        "    data = yf.download(  # or pdr.get_data_yahoo(...\n",
        "    # Yahoo finance symbol\n",
        "    tickers = symbol,\n",
        "\n",
        "    # use \"period\" instead of start/end\n",
        "    # valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max\n",
        "    # (optional, default is '1mo')\n",
        "    period = period,\n",
        "\n",
        "    # fetch data by interval (including intraday if period < 60 days)\n",
        "    # valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
        "    # (optional, default is '1d')\n",
        "    interval = interval,\n",
        "\n",
        "    # group by ticker (to access via data['SPY'])\n",
        "    # (optional, default is 'column')\n",
        "    group_by = 'ticker',\n",
        "\n",
        "    # adjust all OHLC automatically\n",
        "    # (optional, default is False)\n",
        "    auto_adjust = False,\n",
        "\n",
        "    # download pre/post regular market hours data\n",
        "    # (optional, default is False)\n",
        "    prepost = False,\n",
        "\n",
        "    # use threads for mass downloading? (True/False/Integer)\n",
        "    # (optional, default is True)\n",
        "    threads = True,\n",
        "\n",
        "    # proxy URL scheme use use when downloading?\n",
        "    # (optional, default is None)\n",
        "    proxy = None\n",
        "    )\n",
        "\n",
        "    data[\"returns\"] = data['Close'] / data['Close'].shift(1)\n",
        "    data.drop(['Adj Close', 'Volume'], axis='columns', inplace=True)\n",
        "    data.dropna(inplace=True)\n",
        "\n",
        "    return data\n",
        "\n",
        "#Data_Raw = getHistoryYfinance(symbol, interval, period)\n",
        "#Data_Raw.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qq9D1u7Np9W7"
      },
      "outputs": [],
      "source": [
        "def getHistoryOanda(pair = \"EUR_USD\", period = 'M5'):\n",
        "    #columns = ['volume',\t'mid_o',\t'mid_h',\t'mid_l',\t'mid_c', 'time']\n",
        "    columns = ['mid_o',\t'mid_h',\t'mid_l',\t'mid_c', 'time']\n",
        "\n",
        "    if(period == 'H1'):\n",
        "        data = pd.read_pickle(f\"{pair}_H1.pkl\")\n",
        "\n",
        "    if(period == 'M5'):\n",
        "        data = pd.read_pickle(f\"{pair}_M5.pkl\")\n",
        "\n",
        "    data = data[columns]\n",
        "\n",
        "    #dict = {\"time\" : \"Datetime\",  \"volume\" : \"Volume\" , \"mid_o\" : \"Open\" , \"mid_h\" : \"High\", \"mid_l\" : \"Low\" , \"mid_c\" : \"Close\"}\n",
        "    dict = {\"time\" : \"Datetime\", \"mid_o\" : \"Open\" , \"mid_h\" : \"High\", \"mid_l\" : \"Low\" , \"mid_c\" : \"Close\"}\n",
        "\n",
        "    data = data.rename(columns= dict, inplace=False)\n",
        "\n",
        "    data[\"returns\"] = data[\"Close\"] / data[\"Close\"].shift(1) - 1\n",
        "\n",
        "    data = data.dropna()\n",
        "    data = data.set_index(\"Datetime\")\n",
        "    return data\n",
        "\n",
        "\n",
        "Data_Raw = getHistoryOanda()\n",
        "Data_Raw.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOiI_7Q5p9W8"
      },
      "source": [
        "## Display data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPfYeW8bp9W8"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14,5))\n",
        "sns.set_style(\"ticks\")\n",
        "sns.lineplot(data=Data_Raw,x=\"Datetime\",y='Close',color='blue')\n",
        "sns.despine()\n",
        "plt.title(symbol + ' Close Price',size='x-large',color='black')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96-9z80up9W8"
      },
      "source": [
        "## Create features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_GCAx2Vp9W9"
      },
      "outputs": [],
      "source": [
        "def prepare_features(data):\n",
        "    ''' Prepares the feature columns for training set and test set.\n",
        "    '''\n",
        "    feature_columns = []\n",
        "    lags = 5\n",
        "\n",
        "    for lag in range(1, lags + 1):\n",
        "        col = \"lag{}\".format(lag)\n",
        "        data[col] = data[\"returns\"].shift(lag)\n",
        "        feature_columns.append(col)\n",
        "\n",
        "    data['Ask_Dir'] = np.where(data['Close'].shift(-1) > data.Close, 1, 0)\n",
        "    feature_columns.append('Ask_Dir')\n",
        "\n",
        "    data['SMA'] = abstract.SMA(data['Close'], timeperiod=12)\n",
        "    feature_columns.append('SMA')\n",
        "    data['SMA_Dir'] = np.where(data['SMA'].shift(-1) > data.SMA, 1, 0)\n",
        "    feature_columns.append('SMA_Dir')\n",
        "\n",
        "    data['RSI'] = abstract.RSI(data['Close'], timeperiod=12)\n",
        "    feature_columns.append('RSI')\n",
        "    data['RSI_Dir'] = np.where(data['RSI'].shift(-1) > data.RSI, 1, 0)\n",
        "    feature_columns.append('RSI_Dir')\n",
        "\n",
        "    data['fastk'], data['fastd'] = abstract.STOCHRSI(data[\"Close\"], timeperiod=14, fastk_period=5, fastd_period=3, fastd_matype=0)\n",
        "    feature_columns.append('fastk')\n",
        "    feature_columns.append('fastd')\n",
        "    data['fastk_Dir'] = np.where(data['fastk'].shift(-1) > data.fastk, 1, 0)\n",
        "    feature_columns.append('fastk_Dir')\n",
        "    data['fastd_Dir'] = np.where(data['fastd'].shift(-1) > data.fastd, 1, 0)\n",
        "    feature_columns.append('fastd_Dir')\n",
        "\n",
        "    data.dropna(inplace=True)\n",
        "\n",
        "    return data, feature_columns\n",
        "\n",
        "Data_Processed, feature_columns = prepare_features(Data_Raw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3w7F4ABp9W9"
      },
      "outputs": [],
      "source": [
        "Data_Processed.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aol8Vg0Dp9W9"
      },
      "outputs": [],
      "source": [
        "Data_Processed.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btqKjzbDp9W9"
      },
      "outputs": [],
      "source": [
        "Data_Processed.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGoL3u0up9W9"
      },
      "outputs": [],
      "source": [
        "Data_Processed.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlDKBYF4p9W-"
      },
      "outputs": [],
      "source": [
        "print(feature_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_-laeEHp9W-"
      },
      "outputs": [],
      "source": [
        "df = Data_Processed.sort_values(by=['Datetime']).copy()\n",
        "data = pd.DataFrame(df)\n",
        "data_filtered = data[feature_columns]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEfi4S5bp9W-"
      },
      "outputs": [],
      "source": [
        "# We add a prediction column and set dummy values to prepare the data for scaling\n",
        "data_filtered_ext = data_filtered.copy()\n",
        "data_filtered_ext['Prediction'] = df['returns']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WM_5Clqjp9W-"
      },
      "source": [
        "## Split Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZX0sqhop9W-"
      },
      "outputs": [],
      "source": [
        "\n",
        "min_date = data_filtered_ext.index.min()\n",
        "max_date = data_filtered_ext.index.max()\n",
        "print(\"Min:\", min_date, \"Max:\", max_date)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZogo0rzp9W-"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_percent = .75\n",
        "time_between = max_date - min_date\n",
        "train_cutoff = min_date + train_percent*time_between\n",
        "train_cutoff\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2jDXn3gp9W-"
      },
      "outputs": [],
      "source": [
        "\n",
        "df_train = data_filtered_ext[data_filtered_ext.index <= train_cutoff]\n",
        "df_test = data_filtered_ext[data_filtered_ext.index > train_cutoff]\n",
        "print(\"Train:\", df_train.index.min(), df_train.index.max())\n",
        "print(\"Test:\", df_test.index.min(), df_test.index.max())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoUtoGLJp9W-"
      },
      "source": [
        "## Scaling data\n",
        "### Convert to numpy array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUKaWIYep9W-"
      },
      "outputs": [],
      "source": [
        "# Get the number of rows in the data\n",
        "def convert_to_np(pd_data):\n",
        "    nrows = pd_data.shape[0]\n",
        "\n",
        "    # Convert the data to numpy values\n",
        "    np_data_unscaled = np.array(pd_data)\n",
        "    np_data = np.reshape(np_data_unscaled, (nrows, -1))\n",
        "    print(np_data.shape)\n",
        "\n",
        "    return np_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ducGuYbgp9W_"
      },
      "outputs": [],
      "source": [
        "np_x_train = convert_to_np(df_train[feature_columns])\n",
        "np_x_test = convert_to_np(df_test[feature_columns])\n",
        "np_y_train = convert_to_np(df_train['Prediction'])\n",
        "np_y_test = convert_to_np(df_test['Prediction'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETgxSgf1p9W_"
      },
      "source": [
        "### Scale data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNlBlVZLp9W_"
      },
      "outputs": [],
      "source": [
        "# Transform the data by scaling each feature to a range between 0 and 1\n",
        "def scale_np(np_data):\n",
        "    obj_scaler = MinMaxScaler()\n",
        "    np_data_scaled = obj_scaler.fit_transform(np_data)\n",
        "\n",
        "    return np_data_scaled, obj_scaler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbXazrG7p9W_"
      },
      "outputs": [],
      "source": [
        "np_x_train_scale, scaler = scale_np(np_x_train)\n",
        "np_x_test_scale, scaler_data = scale_np(np_x_test)\n",
        "np_y_train_scale, scaler = scale_np(np_y_train)\n",
        "np_y_test_scale, scaler_pred = scale_np(np_y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGBe1oN2p9W_"
      },
      "source": [
        "## Split data into training and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGfssmFLp9W_"
      },
      "outputs": [],
      "source": [
        "index_target = df.columns.get_loc(\"returns\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_12ml6ip9W_"
      },
      "outputs": [],
      "source": [
        "# Prediction Index\n",
        "sequence_length = 5\n",
        "# The RNN needs data with the format of [samples, time steps, features]\n",
        "# Here, we create N samples, sequence_length time steps per sample, and 6 features\n",
        "def partition_x_dataset(sequence_length, data):\n",
        "    x = []\n",
        "    data_len = data.shape[0]\n",
        "    for i in range(sequence_length, data_len):\n",
        "        x.append(data[i-sequence_length:i,:]) #contains sequence_length values 0-sequence_length * columsn\n",
        "\n",
        "    # Convert the x to numpy array\n",
        "    x = np.array(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def partition_y_dataset(sequence_length, data):\n",
        "    y = []\n",
        "    data_len = data.shape[0]\n",
        "    for i in range(sequence_length, data_len):\n",
        "        y.append(data[i, 0]) #contains the prediction values for validation,  for single-step prediction\n",
        "\n",
        "    # Convert the y to numpy array\n",
        "    y = np.array(y)\n",
        "    return y\n",
        "\n",
        "# Generate training data and test data\n",
        "x_train = partition_x_dataset(sequence_length, np_x_train_scale)\n",
        "x_test = partition_x_dataset(sequence_length, np_x_test_scale)\n",
        "y_train = partition_y_dataset(sequence_length, np_y_train_scale)\n",
        "y_test = partition_y_dataset(sequence_length, np_y_test_scale)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkUbKuvHp9W_"
      },
      "outputs": [],
      "source": [
        "len(x_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0E2giShp9W_"
      },
      "outputs": [],
      "source": [
        "# Print the shapes: the result is: (rows, training_sequence, features) (prediction value, )\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-kLHdxap9W_"
      },
      "outputs": [],
      "source": [
        "# Validate that the prediction value and the input match up\n",
        "# The last close price of the second input sample should equal the first prediction value\n",
        "print(x_train[1][sequence_length-1][index_target])\n",
        "print(y_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Z_coWYyp9XA"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULlXWoQAp9XA"
      },
      "outputs": [],
      "source": [
        "# Configure the neural network model\n",
        "model = Sequential()\n",
        "\n",
        "# Model with n_neurons = inputshape Timestamps, each with x_train.shape[2] variables\n",
        "#n_neurons = x_train.shape[1] * x_train.shape[2]\n",
        "n_neurons = 20\n",
        "print(n_neurons, x_train.shape[1], x_train.shape[2])\n",
        "model.add(LSTM(n_neurons, return_sequences=True, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
        "model.add(LSTM(n_neurons, return_sequences=True, activation='relu'))\n",
        "model.add(LSTM(n_neurons, return_sequences=False))\n",
        "model.add(Dense(5))\n",
        "model.add(Dense(1))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTcV_nCxp9XA"
      },
      "outputs": [],
      "source": [
        "# Training the model\n",
        "#epochs = 50\n",
        "epochs = 200\n",
        "batch_size = 16\n",
        "early_stop = EarlyStopping(monitor='loss', patience=5, verbose=1)\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    validation_data=(x_test, y_test)\n",
        "                   )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mj7LPtgnp9XA"
      },
      "outputs": [],
      "source": [
        "# Plot training & validation loss values\n",
        "fig, ax = plt.subplots(figsize=(16, 5), sharex=True)\n",
        "sns.lineplot(data=history.history[\"loss\"])\n",
        "plt.title(\"Model loss\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "ax.xaxis.set_major_locator(plt.MaxNLocator(epochs))\n",
        "plt.legend([\"Train\", \"Test\"], loc=\"upper left\")\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "He1IFBm_p9XA"
      },
      "source": [
        "# Get the predicted values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9NFv1VHp9XE"
      },
      "outputs": [],
      "source": [
        "\n",
        "y_pred_scaled = model.predict(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BKPSAIup9XE"
      },
      "source": [
        "# Unscale the predicted values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8EfYlGFp9XE"
      },
      "outputs": [],
      "source": [
        "# Unscale the predicted values\n",
        "y_pred = scaler_pred.inverse_transform(y_pred_scaled)\n",
        "y_test_unscaled = scaler_pred.inverse_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# Mean Absolute Error (MAE)\n",
        "MAE = mean_absolute_error(y_test_unscaled, y_pred)\n",
        "print(f'Median Absolute Error (MAE): {np.round(MAE, 2)}')\n",
        "\n",
        "# Mean Absolute Percentage Error (MAPE)\n",
        "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
        "print(f'Mean Absolute Percentage Error (MAPE): {np.round(MAPE, 2)} %')\n",
        "\n",
        "# Median Absolute Percentage Error (MDAPE)\n",
        "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled)) ) * 100\n",
        "print(f'Median Absolute Percentage Error (MDAPE): {np.round(MDAPE, 2)} %')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKHhNdpap9XE"
      },
      "outputs": [],
      "source": [
        "df_result = pd.DataFrame(y_test)\n",
        "\n",
        "df_result['pred'] = y_test_unscaled.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TkP_7Svp9XE"
      },
      "outputs": [],
      "source": [
        "df_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8Z7QSsVp9XF"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(x_test)\n",
        "predictions = scaler_pred.inverse_transform(predictions)\n",
        "rmse = np.sqrt(np.mean(predictions - y_test)**2)\n",
        "rmse\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNeYvgSMp9XF"
      },
      "outputs": [],
      "source": [
        "data = Data_Raw.filter(['returns'])\n",
        "train = data[:4420]\n",
        "validation = data[4420:]\n",
        "validation['Predictions'] = predictions\n",
        "plt.figure(figsize=(16,8))\n",
        "plt.title('Model')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Close Price USD ($)')\n",
        "plt.plot(train)\n",
        "plt.plot(validation[['returns', 'Predictions']])\n",
        "plt.legend(['Train', 'Val', 'Predictions'], loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOOThtRQp9XF"
      },
      "outputs": [],
      "source": [
        "y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPQityGOp9XF"
      },
      "outputs": [],
      "source": [
        "y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggxQP0bxp9XF"
      },
      "outputs": [],
      "source": [
        "len(x_train)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "tensor",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}